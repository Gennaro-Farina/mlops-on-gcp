{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using custom containers with AI Platform Training\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Learn how to create a train and a validation split with BigQuery\n",
    "1. Learn how to wrap a machine learning model into a Docker container and train in on AI Platform\n",
    "1. Learn how to use the hyperparameter tunning engine on Google Cloud to find the best hyperparameters\n",
    "1. Learn how to deploy a trained machine learning model Google Cloud as a rest API and query it\n",
    "\n",
    "In this lab, you develop a  multi-class classification model, package the model as a docker image, and run on **AI Platform Training** as a training application. The training application trains a multi-class classification model that predicts the type of forest cover from cartographic data. The [dataset](../../../datasets/covertype/README.md) used in the lab is based on **Covertype Data Set** from UCI Machine Learning Repository.\n",
    "\n",
    "Scikit-learn is one of the most useful libraries for machine learning in Python. The training code uses `scikit-learn` for data pre-processing and modeling. \n",
    "\n",
    "The code is instrumented using the `hypertune` package so it can be used with **AI Platform** hyperparameter tuning job in searching for the best combination of hyperparameter values by optimizing the metrics you specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command in the cell below to install gcsfs package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gcsfs==0.8\n",
      "  Downloading gcsfs-0.8.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.8) (2.28.2)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.8) (2023.1.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.8) (5.1.1)\n",
      "Collecting ujson\n",
      "  Downloading ujson-5.7.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.8) (2.16.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.8) (3.8.4)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.8) (1.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs==0.8) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs==0.8) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs==0.8) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs==0.8) (4.9)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.8) (6.0.4)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.8) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.8) (4.5.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.8) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.8) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.8) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.8) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.8) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.8) (1.8.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib->gcsfs==0.8) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.8) (1.24.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.8) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.8) (2022.12.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==0.8) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.8) (3.2.2)\n",
      "Installing collected packages: ujson, gcsfs\n",
      "  Attempting uninstall: gcsfs\n",
      "    Found existing installation: gcsfs 2023.1.0\n",
      "    Uninstalling gcsfs-2023.1.0:\n",
      "      Successfully uninstalled gcsfs-2023.1.0\n",
      "Successfully installed gcsfs-0.8.0 ujson-5.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gcsfs==0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare lab dataset\n",
    "\n",
    "Set environment variable so that we can use them throughout the entire lab.\n",
    "\n",
    "The pipeline ingests data from BigQuery. The cell below uploads the Covertype dataset to BigQuery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID=!(gcloud config get-value core/project)\n",
    "PROJECT_ID=PROJECT_ID[0]\n",
    "DATASET_ID='covertype_dataset'\n",
    "DATASET_LOCATION='US'\n",
    "TABLE_ID='covertype'\n",
    "DATA_SOURCE='gs://cloud-training/OCBL203/workshop-datasets/dataset.csv'\n",
    "SCHEMA='Elevation:INTEGER,Aspect:INTEGER,Slope:INTEGER,Horizontal_Distance_To_Hydrology:INTEGER,Vertical_Distance_To_Hydrology:INTEGER,Horizontal_Distance_To_Roadways:INTEGER,Hillshade_9am:INTEGER,Hillshade_Noon:INTEGER,Hillshade_3pm:INTEGER,Horizontal_Distance_To_Fire_Points:INTEGER,Wilderness_Area:STRING,Soil_Type:STRING,Cover_Type:INTEGER'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the BigQuery dataset and upload the Covertype csv data into a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'qwiklabs-gcp-03-b8978ad3fa8f:covertype_dataset' successfully created.\n"
     ]
    }
   ],
   "source": [
    "!bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r4740838d30a45d28_00000187323609c3_1 ... (2s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set location paths, connections strings, and other environment settings. Make sure to update   `REGION`, and `ARTIFACT_STORE`  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for AI Platform Training and Prediction\n",
    "- `ARTIFACT_STORE` - the Cloud Storage bucket created during installation of AI Platform Pipelines. The bucket name starts with the `qwiklabs-gcp-xx-xxxxxxx-kubeflowpipelines-default` prefix.\n",
    "\n",
    "Run gsutil ls without URLs to list all of the Cloud Storage buckets under your default project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.qwiklabs-gcp-03-b8978ad3fa8f.appspot.com/\n",
      "gs://qwiklabs-gcp-03-b8978ad3fa8f-kubeflowpipelines-default/\n",
      "gs://qwiklabs-gcp-03-b8978ad3fa8f_cloudbuild/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HINT: For ARTIFACT_STORE, copy the bucket name which starts with the qwiklabs-gcp-xx-xxxxxxx-kubeflowpipelines-default prefix from the previous cell output.\n",
    "\n",
    "Your copied value should look like 'gs://qwiklabs-gcp-xx-xxxxxxx-kubeflowpipelines-default')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://qwiklabs-gcp-03-b8978ad3fa8f-kubeflowpipelines-default' # TO DO: REPLACE WITH YOUR ARTIFACT_STORE NAME\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'dataset.csv')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Covertype dataset \n",
    "\n",
    "Run the query statement below to scan covertype_dataset.covertype table in BigQuery and return the computed result rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbdb1b1093e401ca9b17c251e03a75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f74eeb787f465589694a509386e678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2085</td>\n",
       "      <td>256</td>\n",
       "      <td>18</td>\n",
       "      <td>150</td>\n",
       "      <td>27</td>\n",
       "      <td>738</td>\n",
       "      <td>176</td>\n",
       "      <td>248</td>\n",
       "      <td>208</td>\n",
       "      <td>914</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2702</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2125</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>871</td>\n",
       "      <td>169</td>\n",
       "      <td>248</td>\n",
       "      <td>215</td>\n",
       "      <td>300</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2702</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2146</td>\n",
       "      <td>256</td>\n",
       "      <td>34</td>\n",
       "      <td>150</td>\n",
       "      <td>62</td>\n",
       "      <td>1253</td>\n",
       "      <td>122</td>\n",
       "      <td>237</td>\n",
       "      <td>239</td>\n",
       "      <td>511</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2702</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2186</td>\n",
       "      <td>256</td>\n",
       "      <td>38</td>\n",
       "      <td>210</td>\n",
       "      <td>102</td>\n",
       "      <td>1294</td>\n",
       "      <td>109</td>\n",
       "      <td>232</td>\n",
       "      <td>244</td>\n",
       "      <td>552</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2702</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2831</td>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "      <td>277</td>\n",
       "      <td>183</td>\n",
       "      <td>1706</td>\n",
       "      <td>153</td>\n",
       "      <td>246</td>\n",
       "      <td>225</td>\n",
       "      <td>1485</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>3136</td>\n",
       "      <td>254</td>\n",
       "      <td>12</td>\n",
       "      <td>319</td>\n",
       "      <td>60</td>\n",
       "      <td>5734</td>\n",
       "      <td>193</td>\n",
       "      <td>248</td>\n",
       "      <td>193</td>\n",
       "      <td>2467</td>\n",
       "      <td>Rawah</td>\n",
       "      <td>C7746</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>3242</td>\n",
       "      <td>254</td>\n",
       "      <td>12</td>\n",
       "      <td>636</td>\n",
       "      <td>148</td>\n",
       "      <td>3551</td>\n",
       "      <td>193</td>\n",
       "      <td>248</td>\n",
       "      <td>193</td>\n",
       "      <td>2010</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C7757</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>2071</td>\n",
       "      <td>255</td>\n",
       "      <td>12</td>\n",
       "      <td>234</td>\n",
       "      <td>63</td>\n",
       "      <td>342</td>\n",
       "      <td>192</td>\n",
       "      <td>247</td>\n",
       "      <td>193</td>\n",
       "      <td>247</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2706</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>3248</td>\n",
       "      <td>255</td>\n",
       "      <td>12</td>\n",
       "      <td>730</td>\n",
       "      <td>113</td>\n",
       "      <td>725</td>\n",
       "      <td>192</td>\n",
       "      <td>247</td>\n",
       "      <td>193</td>\n",
       "      <td>2724</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C7756</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>3153</td>\n",
       "      <td>255</td>\n",
       "      <td>12</td>\n",
       "      <td>404</td>\n",
       "      <td>116</td>\n",
       "      <td>2139</td>\n",
       "      <td>192</td>\n",
       "      <td>247</td>\n",
       "      <td>193</td>\n",
       "      <td>994</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C7756</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0           2085     256     18                               150   \n",
       "1           2125     256     20                                30   \n",
       "2           2146     256     34                               150   \n",
       "3           2186     256     38                               210   \n",
       "4           2831     256     25                               277   \n",
       "...          ...     ...    ...                               ...   \n",
       "99995       3136     254     12                               319   \n",
       "99996       3242     254     12                               636   \n",
       "99997       2071     255     12                               234   \n",
       "99998       3248     255     12                               730   \n",
       "99999       3153     255     12                               404   \n",
       "\n",
       "       Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                                  27                              738   \n",
       "1                                  12                              871   \n",
       "2                                  62                             1253   \n",
       "3                                 102                             1294   \n",
       "4                                 183                             1706   \n",
       "...                               ...                              ...   \n",
       "99995                              60                             5734   \n",
       "99996                             148                             3551   \n",
       "99997                              63                              342   \n",
       "99998                             113                              725   \n",
       "99999                             116                             2139   \n",
       "\n",
       "       Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0                176             248            208   \n",
       "1                169             248            215   \n",
       "2                122             237            239   \n",
       "3                109             232            244   \n",
       "4                153             246            225   \n",
       "...              ...             ...            ...   \n",
       "99995            193             248            193   \n",
       "99996            193             248            193   \n",
       "99997            192             247            193   \n",
       "99998            192             247            193   \n",
       "99999            192             247            193   \n",
       "\n",
       "       Horizontal_Distance_To_Fire_Points Wilderness_Area Soil_Type  \\\n",
       "0                                     914           Cache     C2702   \n",
       "1                                     300           Cache     C2702   \n",
       "2                                     511           Cache     C2702   \n",
       "3                                     552           Cache     C2702   \n",
       "4                                    1485       Commanche     C2705   \n",
       "...                                   ...             ...       ...   \n",
       "99995                                2467           Rawah     C7746   \n",
       "99996                                2010       Commanche     C7757   \n",
       "99997                                 247           Cache     C2706   \n",
       "99998                                2724       Commanche     C7756   \n",
       "99999                                 994       Commanche     C7756   \n",
       "\n",
       "       Cover_Type  \n",
       "0               5  \n",
       "1               2  \n",
       "2               2  \n",
       "3               2  \n",
       "4               1  \n",
       "...           ...  \n",
       "99995           1  \n",
       "99996           0  \n",
       "99997           2  \n",
       "99998           1  \n",
       "99999           1  \n",
       "\n",
       "[100000 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT *\n",
    "FROM `covertype_dataset.covertype`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation splits\n",
    "\n",
    "Use BigQuery to sample training and validation splits and save them to Cloud Storage.\n",
    "\n",
    "### Create a training split\n",
    "\n",
    "Run the query below in order to have repeatable sampling of the data in BigQuery. Note that `FARM_FINGERPRINT()` is used on the field that you are going to split your data. It creates a training split that takes 80% of the data using the `bq` command and exports this split into the BigQuery table of `covertype_dataset.training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r51528afff1dad0f_0000018732403a08_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.training \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (1, 2, 3, 4)' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `bq` extract command to export the BigQuery training table to GCS at `$TRAINING_FILE_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r541418ed2e7cc2ee_0000018732405498_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.training \\\n",
    "$TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the first cell below, create \n",
    "a validation split that takes 10% of the data using the `bq` command and\n",
    "export this split into the BigQuery table `covertype_dataset.validation`.\n",
    "\n",
    "In the second cell, use the `bq` command to export that BigQuery validation table to GCS at `$VALIDATION_FILE_PATH`.\n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-01-caip-containers** and opening **lab-01.ipynb**.\n",
    "</ql-infobox>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r653e1b2ccea051b7_0000018732413610_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.validation \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (1, 2, 3, 4)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r43bd15e70052bf9f_00000187324189d7_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "# TO DO: Your code goes here to export the validation table to the Cloud Storage bucket.\n",
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.training \\\n",
    "$VALIDATION_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40009, 13)\n",
      "(40009, 13)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "print(df_train.shape)\n",
    "print(df_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a training application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the `sklearn` training pipeline.\n",
    "\n",
    "The training pipeline preprocesses data by standardizing all numeric features using `sklearn.preprocessing.StandardScaler` and encoding all categorical features using `sklearn.preprocessing.OneHotEncoder`. It uses stochastic gradient descent linear classifier (`SGDClassifier`) for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_indexes = slice(0, 10)\n",
    "categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log', tol=1e-3))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all numeric features to `float64`\n",
    "\n",
    "To avoid warning messages from `StandardScaler` all numeric features are converted to `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "\n",
    "df_train = df_train.astype(num_features_type_map)\n",
    "df_validation = df_validation.astype(num_features_type_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                                  slice(0, 10, None)),\n",
       "                                                 ('cat', OneHotEncoder(),\n",
       "                                                  slice(10, 12, None))])),\n",
       "                ('classifier',\n",
       "                 SGDClassifier(alpha=0.001, loss='log', max_iter=200))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train.drop('Cover_Type', axis=1)\n",
    "y_train = df_train['Cover_Type']\n",
    "X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "y_validation = df_validation['Cover_Type']\n",
    "\n",
    "pipeline.set_params(classifier__alpha=0.001, classifier__max_iter=200)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the trained model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7014421755105101\n"
     ]
    }
   ],
   "source": [
    "accuracy = pipeline.score(X_validation, y_validation)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the hyperparameter tuning application.\n",
    "Since the training run on this dataset is computationally expensive you can benefit from running a distributed hyperparameter tuning job on AI Platform Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the tuning script. \n",
    "\n",
    "Notice the use of the `hypertune` package to report the `accuracy` optimization metric to AI Platform hyperparameter tuning service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the code below to capture the metric that the hyper parameter tunning engine will use to optimize\n",
    "the hyper parameter. \n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-01-caip-containers** and opening **lab-01.ipynb**.\n",
    "</ql-infobox>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, \n",
    "                   validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_feature_indexes = slice(0, 10)\n",
    "    categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', SGDClassifier(loss='log',tol=1e-3))\n",
    "    ])\n",
    "\n",
    "    num_features_type_map = {feature: 'float64' for feature \n",
    "                             in df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('Cover_Type', axis=1)\n",
    "    y_train = df_train['Cover_Type']\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "    # TO DO: Your code goes here to score the model with the validation data and capture the result\n",
    "    # with the hypertune library\n",
    "    X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "    y_validation = df_validation['Cover_Type']\n",
    "    accuracy = pipeline.score(X_validation, y_validation)\n",
    "    print('Model accuracy: {}'.format(accuracy))\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='accuracy',\n",
    "        metric_value=accuracy\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path],\n",
    "                          stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "Notice that we are installing specific versions of `scikit-learn` and `pandas` in the training image. This is done to make sure that the training runtime is aligned with the serving runtime. Later in the notebook you will deploy the model to AI Platform Prediction, using the 1.15 version of AI Platform Prediction runtime. \n",
    "\n",
    "Make sure to update the URI for the base image so that it points to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the Dockerfile below so that it copies the 'train.py' file into the container\n",
    "at `/app` and runs it when the container is started. \n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-01-caip-containers** and opening **lab-01.ipynb**.\n",
    "</ql-infobox>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT ['python', 'train.py']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the docker image. \n",
    "\n",
    "You use **Cloud Build** to build the image and push it your project's **Container Registry**. As you use the remote cloud service to build the image, you don't need a local installation of Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 3.3 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://qwiklabs-gcp-03-b8978ad3fa8f_cloudbuild/source/1680175738.932265-dac672583d7b4ad5b466b5e29a4cc0c7.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-03-b8978ad3fa8f/locations/global/builds/1c00c05b-8312-4249-a745-8e1921342c06].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/1c00c05b-8312-4249-a745-8e1921342c06?project=354076792935 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"1c00c05b-8312-4249-a745-8e1921342c06\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-03-b8978ad3fa8f_cloudbuild/source/1680175738.932265-dac672583d7b4ad5b466b5e29a4cc0c7.tgz#1680175739162995\n",
      "Copying gs://qwiklabs-gcp-03-b8978ad3fa8f_cloudbuild/source/1680175738.932265-dac672583d7b4ad5b466b5e29a4cc0c7.tgz#1680175739162995...\n",
      "/ [1 files][  1.6 KiB/  1.6 KiB]                                                \n",
      "Operation completed over 1 objects/1.6 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "47c764472391: Already exists\n",
      "3b4c9936d49d: Pulling fs layer\n",
      "93d0bde73b80: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "ea0acd06b583: Pulling fs layer\n",
      "aa8facca7254: Pulling fs layer\n",
      "7b3b020a3528: Pulling fs layer\n",
      "bd3fc45f53bc: Pulling fs layer\n",
      "f24ea979b0a7: Pulling fs layer\n",
      "ec1295410f17: Pulling fs layer\n",
      "7dde9985aedd: Pulling fs layer\n",
      "e2a128ccb651: Pulling fs layer\n",
      "bbb061b8a3ff: Pulling fs layer\n",
      "4c0368833e86: Pulling fs layer\n",
      "5174bf6219f1: Pulling fs layer\n",
      "f9f17923d08d: Pulling fs layer\n",
      "0c3428ecf4ca: Pulling fs layer\n",
      "c080dc6d97d3: Pulling fs layer\n",
      "261fcec10815: Pulling fs layer\n",
      "afcf4d15e79e: Pulling fs layer\n",
      "98ebe55c95ab: Pulling fs layer\n",
      "e0d7d84f2a54: Pulling fs layer\n",
      "aca671ae4bb6: Pulling fs layer\n",
      "ea0acd06b583: Waiting\n",
      "aa8facca7254: Waiting\n",
      "7b3b020a3528: Waiting\n",
      "bd3fc45f53bc: Waiting\n",
      "f24ea979b0a7: Waiting\n",
      "ec1295410f17: Waiting\n",
      "7dde9985aedd: Waiting\n",
      "e2a128ccb651: Waiting\n",
      "bbb061b8a3ff: Waiting\n",
      "4c0368833e86: Waiting\n",
      "5174bf6219f1: Waiting\n",
      "f9f17923d08d: Waiting\n",
      "0c3428ecf4ca: Waiting\n",
      "c080dc6d97d3: Waiting\n",
      "261fcec10815: Waiting\n",
      "afcf4d15e79e: Waiting\n",
      "98ebe55c95ab: Waiting\n",
      "e0d7d84f2a54: Waiting\n",
      "aca671ae4bb6: Waiting\n",
      "93d0bde73b80: Verifying Checksum\n",
      "93d0bde73b80: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "3b4c9936d49d: Verifying Checksum\n",
      "3b4c9936d49d: Download complete\n",
      "7b3b020a3528: Verifying Checksum\n",
      "7b3b020a3528: Download complete\n",
      "bd3fc45f53bc: Verifying Checksum\n",
      "bd3fc45f53bc: Download complete\n",
      "f24ea979b0a7: Verifying Checksum\n",
      "f24ea979b0a7: Download complete\n",
      "aa8facca7254: Verifying Checksum\n",
      "aa8facca7254: Download complete\n",
      "7dde9985aedd: Verifying Checksum\n",
      "7dde9985aedd: Download complete\n",
      "e2a128ccb651: Verifying Checksum\n",
      "e2a128ccb651: Download complete\n",
      "bbb061b8a3ff: Verifying Checksum\n",
      "bbb061b8a3ff: Download complete\n",
      "4c0368833e86: Verifying Checksum\n",
      "4c0368833e86: Download complete\n",
      "5174bf6219f1: Download complete\n",
      "ec1295410f17: Verifying Checksum\n",
      "ec1295410f17: Download complete\n",
      "f9f17923d08d: Verifying Checksum\n",
      "f9f17923d08d: Download complete\n",
      "0c3428ecf4ca: Verifying Checksum\n",
      "0c3428ecf4ca: Download complete\n",
      "c080dc6d97d3: Verifying Checksum\n",
      "c080dc6d97d3: Download complete\n",
      "261fcec10815: Verifying Checksum\n",
      "261fcec10815: Download complete\n",
      "3b4c9936d49d: Pull complete\n",
      "afcf4d15e79e: Verifying Checksum\n",
      "afcf4d15e79e: Download complete\n",
      "93d0bde73b80: Pull complete\n",
      "ea0acd06b583: Verifying Checksum\n",
      "ea0acd06b583: Download complete\n",
      "aca671ae4bb6: Verifying Checksum\n",
      "aca671ae4bb6: Download complete\n",
      "4f4fb700ef54: Pull complete\n",
      "e0d7d84f2a54: Verifying Checksum\n",
      "e0d7d84f2a54: Download complete\n",
      "98ebe55c95ab: Verifying Checksum\n",
      "98ebe55c95ab: Download complete\n",
      "ea0acd06b583: Pull complete\n",
      "aa8facca7254: Pull complete\n",
      "7b3b020a3528: Pull complete\n",
      "bd3fc45f53bc: Pull complete\n",
      "f24ea979b0a7: Pull complete\n",
      "ec1295410f17: Pull complete\n",
      "7dde9985aedd: Pull complete\n",
      "e2a128ccb651: Pull complete\n",
      "bbb061b8a3ff: Pull complete\n",
      "4c0368833e86: Pull complete\n",
      "5174bf6219f1: Pull complete\n",
      "f9f17923d08d: Pull complete\n",
      "0c3428ecf4ca: Pull complete\n",
      "c080dc6d97d3: Pull complete\n",
      "261fcec10815: Pull complete\n",
      "afcf4d15e79e: Pull complete\n",
      "98ebe55c95ab: Pull complete\n",
      "e0d7d84f2a54: Pull complete\n",
      "aca671ae4bb6: Pull complete\n",
      "Digest: sha256:fdd40dc64bb0738b76a3fbd326fea842e9f7eeedb04d1883210efeb30d5bf728\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d3a691d8b72b\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in cc4b60aa2090\n",
      "Collecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 4.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 16.7 MB/s eta 0:00:00\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 31.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2022.7.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=6ef418744281d5086abda1485923f75e6c252afeb5555a7c83a868a755bee6d5\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/97/e1/dd2c472bebcdcaa85fdc07d0f19020299f1c86773028860c53\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3973 sha256=f3f5eb5ff08bfc3e1c36fb59e034798353e4fd4c19c43a6d08364f17e259a3c0\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built fire cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune, termcolor, scikit-learn, pandas, fire\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.1.0 requires pandas!=1.4.0,<1.6,>1.1, but you have pandas 0.24.2 which is incompatible.\n",
      "visions 0.7.5 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "statsmodels 0.13.5 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "seaborn 0.12.2 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.12.3 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fire-0.5.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-2.2.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container cc4b60aa2090\n",
      " ---> 60531e3b50c6\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 5a6948260e56\n",
      "Removing intermediate container 5a6948260e56\n",
      " ---> 60ee6d0fc86e\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 9a75e8215fba\n",
      "Step 5/5 : ENTRYPOINT ['python', 'train.py']\n",
      " ---> Running in d1e9bbaa09f6\n",
      "Removing intermediate container d1e9bbaa09f6\n",
      " ---> c93e13b85596\n",
      "Successfully built c93e13b85596\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-03-b8978ad3fa8f/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-03-b8978ad3fa8f/trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-03-b8978ad3fa8f/trainer_image]\n",
      "303738f32fef: Preparing\n",
      "aaa2dbde25da: Preparing\n",
      "82cd97cce422: Preparing\n",
      "19e51cd8292e: Preparing\n",
      "e75349e7ebd6: Preparing\n",
      "00456d7694a3: Preparing\n",
      "dcf153d49379: Preparing\n",
      "5f4c5f7f9a88: Preparing\n",
      "d4c7adc674b9: Preparing\n",
      "25b672beea00: Preparing\n",
      "c381da37ef45: Preparing\n",
      "bbf4bedf16ab: Preparing\n",
      "b9ed9dc50710: Preparing\n",
      "1d664bb87d46: Preparing\n",
      "1c73d996bda1: Preparing\n",
      "a472ae45df62: Preparing\n",
      "efc1998725ed: Preparing\n",
      "263b3cc9fb4c: Preparing\n",
      "2e8ff0fc8729: Preparing\n",
      "7728ec2832ab: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "998dc2d46e33: Preparing\n",
      "89d0d239dcf7: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "1e6620f29653: Preparing\n",
      "d60a43dc20a0: Preparing\n",
      "6021993d84a2: Preparing\n",
      "00456d7694a3: Waiting\n",
      "dcf153d49379: Waiting\n",
      "5f4c5f7f9a88: Waiting\n",
      "d4c7adc674b9: Waiting\n",
      "25b672beea00: Waiting\n",
      "c381da37ef45: Waiting\n",
      "bbf4bedf16ab: Waiting\n",
      "b9ed9dc50710: Waiting\n",
      "1d664bb87d46: Waiting\n",
      "1c73d996bda1: Waiting\n",
      "a472ae45df62: Waiting\n",
      "998dc2d46e33: Waiting\n",
      "89d0d239dcf7: Waiting\n",
      "1e6620f29653: Waiting\n",
      "d60a43dc20a0: Waiting\n",
      "6021993d84a2: Waiting\n",
      "efc1998725ed: Waiting\n",
      "263b3cc9fb4c: Waiting\n",
      "2e8ff0fc8729: Waiting\n",
      "7728ec2832ab: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "19e51cd8292e: Layer already exists\n",
      "e75349e7ebd6: Layer already exists\n",
      "dcf153d49379: Layer already exists\n",
      "00456d7694a3: Layer already exists\n",
      "5f4c5f7f9a88: Layer already exists\n",
      "d4c7adc674b9: Layer already exists\n",
      "c381da37ef45: Layer already exists\n",
      "25b672beea00: Layer already exists\n",
      "bbf4bedf16ab: Layer already exists\n",
      "b9ed9dc50710: Layer already exists\n",
      "1c73d996bda1: Layer already exists\n",
      "1d664bb87d46: Layer already exists\n",
      "a472ae45df62: Layer already exists\n",
      "efc1998725ed: Layer already exists\n",
      "263b3cc9fb4c: Layer already exists\n",
      "2e8ff0fc8729: Layer already exists\n",
      "7728ec2832ab: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "998dc2d46e33: Layer already exists\n",
      "89d0d239dcf7: Layer already exists\n",
      "1e6620f29653: Layer already exists\n",
      "d60a43dc20a0: Layer already exists\n",
      "6021993d84a2: Layer already exists\n",
      "303738f32fef: Pushed\n",
      "aaa2dbde25da: Pushed\n",
      "82cd97cce422: Pushed\n",
      "latest: digest: sha256:8dcc76d4833f4d9b1385c8841d794a0abe83e7bc9123944d54fa6e14c51ed39f size: 5967\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                       STATUS\n",
      "1c00c05b-8312-4249-a745-8e1921342c06  2023-03-30T11:28:59+00:00  3M48S     gs://qwiklabs-gcp-03-b8978ad3fa8f_cloudbuild/source/1680175738.932265-dac672583d7b4ad5b466b5e29a4cc0c7.tgz  gcr.io/qwiklabs-gcp-03-b8978ad3fa8f/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit an AI Platform hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the hyperparameter configuration file. \n",
    "Recall that the training code uses `SGDClassifier`. The training application has been designed to accept two hyperparameters that control `SGDClassifier`:\n",
    "- Max iterations\n",
    "- Alpha\n",
    "\n",
    "The below file configures AI Platform hypertuning to run up to 6 trials on up to three nodes and to choose from two discrete values of `max_iter` and the linear range betwee 0.00001 and 0.001 for `alpha`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the `hptuning_config.yaml` file below so that the hyperparameter\n",
    "tunning engine try for parameter values\n",
    "* `max_iter` the two values 200 and 300\n",
    "* `alpha` a linear range of values between 0.00001 and 0.001\n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-01-caip-containers** and opening **lab-01.ipynb**.\n",
    "</ql-infobox>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "trainingInput:\n",
    "    hyperparameters:\n",
    "        goal: MAXIMIZE\n",
    "        maxTrials: 4\n",
    "        maxParallelTrials: 4\n",
    "        hyperparameterMetricTag: accuracy\n",
    "        enableTrialEarlyStopping: TRUE \n",
    "        params:\n",
    "        - parameterName: max_iter\n",
    "          type: DISCRETE\n",
    "          discreteValues: [\n",
    "              200,\n",
    "              300\n",
    "          ]\n",
    "        - parameterName: alpha\n",
    "          type: DOUBLE\n",
    "          minValue: 0.00001\n",
    "          maxValue: 0.001\n",
    "          scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the hyperparameter tuning job.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "Use the `gcloud` command to start the hyperparameter tuning job.\n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-01-caip-containers** and opening **lab-01.ipynb**.\n",
    "</ql-infobox>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20230330_114705] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20230330_114705\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20230330_114705\n",
      "jobId: JOB_20230330_114705\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset_path= $TRAINING_FILE_PATH \\\n",
    "--validation_dataset_path= $VALIDATION_FILE_PATH \\\n",
    "--hptune\n",
    "# TO DO: Complete the command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the job.\n",
    "\n",
    "You can monitor the job using Google Cloud console or from within the notebook using `gcloud` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2023-03-30T11:47:08Z'\n",
      "etag: k-S6vWW0Yn4=\n",
      "jobId: JOB_20230330_114705\n",
      "jobPosition: '0'\n",
      "startTime: '2023-03-30T11:47:13Z'\n",
      "state: RUNNING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --training_dataset_path=\n",
      "  - gs://qwiklabs-gcp-03-b8978ad3fa8f-kubeflowpipelines-default/data/training/dataset.csv\n",
      "  - --validation_dataset_path=\n",
      "  - gs://qwiklabs-gcp-03-b8978ad3fa8f-kubeflowpipelines-default/data/validation/dataset.csv\n",
      "  - --hptune\n",
      "  hyperparameters:\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: accuracy\n",
      "    maxParallelTrials: 4\n",
      "    maxTrials: 4\n",
      "    params:\n",
      "    - discreteValues:\n",
      "      - 200.0\n",
      "      - 300.0\n",
      "      parameterName: max_iter\n",
      "      type: DISCRETE\n",
      "    - maxValue: 0.001\n",
      "      minValue: 1e-05\n",
      "      parameterName: alpha\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: DOUBLE\n",
      "  jobDir: gs://qwiklabs-gcp-03-b8978ad3fa8f-kubeflowpipelines-default/jobs/JOB_20230330_114705\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/qwiklabs-gcp-03-b8978ad3fa8f/trainer_image:latest\n",
      "  region: us-central1\n",
      "trainingOutput:\n",
      "  isHyperparameterTuningJob: true\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20230330_114705?project=qwiklabs-gcp-03-b8978ad3fa8f\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2FJOB_20230330_114705&project=qwiklabs-gcp-03-b8978ad3fa8f\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2023-03-30 11:47:07 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2023-03-30 11:47:08 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2023-03-30 11:47:08 +0000\tservice\t\tJob JOB_20230330_114705 is queued.\n",
      "INFO\t2023-03-30 11:47:19 +0000\tservice\t4\tWaiting for job to be provisioned.\n",
      "INFO\t2023-03-30 11:47:19 +0000\tservice\t1\tWaiting for job to be provisioned.\n",
      "INFO\t2023-03-30 11:47:19 +0000\tservice\t2\tWaiting for job to be provisioned.\n",
      "INFO\t2023-03-30 11:47:19 +0000\tservice\t3\tWaiting for job to be provisioned.\n",
      "INFO\t2023-03-30 11:47:20 +0000\tservice\t3\tWaiting for training program to start.\n",
      "INFO\t2023-03-30 11:47:20 +0000\tservice\t4\tWaiting for training program to start.\n",
      "INFO\t2023-03-30 11:47:21 +0000\tservice\t1\tWaiting for training program to start.\n",
      "INFO\t2023-03-30 11:47:21 +0000\tservice\t2\tWaiting for training program to start.\n",
      "NOTICE\t2023-03-30 11:47:53 +0000\tmaster-replica-0.gcsfuse\t4\tOpening GCS connection...\n",
      "NOTICE\t2023-03-30 11:47:53 +0000\tmaster-replica-0.gcsfuse\t4\tMounting file system \"gcsfuse\"...\n",
      "NOTICE\t2023-03-30 11:47:53 +0000\tmaster-replica-0.gcsfuse\t4\tFile system has been successfully mounted.\n",
      "NOTICE\t2023-03-30 11:47:54 +0000\tmaster-replica-0.gcsfuse\t3\tOpening GCS connection...\n",
      "NOTICE\t2023-03-30 11:47:54 +0000\tmaster-replica-0.gcsfuse\t3\tMounting file system \"gcsfuse\"...\n",
      "NOTICE\t2023-03-30 11:47:54 +0000\tmaster-replica-0.gcsfuse\t3\tFile system has been successfully mounted.\n",
      "NOTICE\t2023-03-30 11:48:02 +0000\tmaster-replica-0.gcsfuse\t2\tOpening GCS connection...\n",
      "NOTICE\t2023-03-30 11:48:02 +0000\tmaster-replica-0.gcsfuse\t2\tMounting file system \"gcsfuse\"...\n",
      "NOTICE\t2023-03-30 11:48:02 +0000\tmaster-replica-0.gcsfuse\t2\tFile system has been successfully mounted.\n",
      "ERROR\t2023-03-30 11:49:38 +0000\tmaster-replica-0\t4\t--training_dataset_path=: 1: [python,: not found\n",
      "ERROR\t2023-03-30 11:49:43 +0000\tmaster-replica-0\t3\t--training_dataset_path=: 1: [python,: not found\n",
      "ERROR\t2023-03-30 11:49:46 +0000\tmaster-replica-0\t2\t--training_dataset_path=: 1: [python,: not found\n",
      "ERROR\t2023-03-30 11:50:02 +0000\tservice\t3\tThe replica master 0 exited with a non-zero status of 127. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=354076792935&resource=ml_job%2Fjob_id%2FJOB_20230330_114705&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22JOB_20230330_114705%22\n",
      "ERROR\t2023-03-30 11:50:03 +0000\tservice\t4\tThe replica master 0 exited with a non-zero status of 127. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=354076792935&resource=ml_job%2Fjob_id%2FJOB_20230330_114705&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22JOB_20230330_114705%22\n",
      "ERROR\t2023-03-30 11:50:13 +0000\tservice\t1\tThe replica master 0 exited with a non-zero status of 127. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=354076792935&resource=ml_job%2Fjob_id%2FJOB_20230330_114705&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22JOB_20230330_114705%22\n",
      "ERROR\t2023-03-30 11:50:14 +0000\tservice\t2\tThe replica master 0 exited with a non-zero status of 127. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=354076792935&resource=ml_job%2Fjob_id%2FJOB_20230330_114705&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22JOB_20230330_114705%22\n",
      "INFO\t2023-03-30 11:55:34 +0000\tservice\t3\tJob failed.\n",
      "INFO\t2023-03-30 11:55:34 +0000\tservice\t4\tJob failed.\n",
      "INFO\t2023-03-30 11:55:45 +0000\tservice\t1\tJob failed.\n",
      "INFO\t2023-03-30 11:55:45 +0000\tservice\t2\tJob failed.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: The above AI platform job stream logs will take approximately 5~10 minutes to display.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job completes you can review the results using Google Cloud Console or programatically by calling the AI Platform Training REST end-point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20230330_114705',\n",
       " 'trainingInput': {'args': ['--training_dataset_path=',\n",
       "   'gs://qwiklabs-gcp-03-b8978ad3fa8f-kubeflowpipelines-default/data/training/dataset.csv',\n",
       "   '--validation_dataset_path=',\n",
       "   'gs://qwiklabs-gcp-03-b8978ad3fa8f-kubeflowpipelines-default/data/validation/dataset.csv',\n",
       "   '--hptune'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'max_iter',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [200, 300]},\n",
       "    {'parameterName': 'alpha',\n",
       "     'minValue': 1e-05,\n",
       "     'maxValue': 0.001,\n",
       "     'type': 'DOUBLE',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'}],\n",
       "   'maxTrials': 4,\n",
       "   'maxParallelTrials': 4,\n",
       "   'hyperparameterMetricTag': 'accuracy',\n",
       "   'enableTrialEarlyStopping': True},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://qwiklabs-gcp-03-b8978ad3fa8f-kubeflowpipelines-default/jobs/JOB_20230330_114705',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/qwiklabs-gcp-03-b8978ad3fa8f/trainer_image:latest'}},\n",
       " 'createTime': '2023-03-30T11:47:08Z',\n",
       " 'startTime': '2023-03-30T11:47:13Z',\n",
       " 'endTime': '2023-03-30T11:55:54Z',\n",
       " 'state': 'FAILED',\n",
       " 'errorMessage': \"Hyperparameter Tuning Trial #1 Failed before any other successful trials were completed. The failed trial had parameters: max_iter=300, alpha=0.000505, .  The trial's error message was: The replica master 0 exited with a non-zero status of 127. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=354076792935&resource=ml_job%2Fjob_id%2FJOB_20230330_114705&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22JOB_20230330_114705%22\",\n",
       " 'trainingOutput': {'trials': [{'trialId': '1',\n",
       "    'hyperparameters': {'alpha': '0.000505', 'max_iter': '300'},\n",
       "    'startTime': '2023-03-30T11:47:19.782241508Z',\n",
       "    'endTime': '2023-03-30T11:55:45Z',\n",
       "    'state': 'FAILED'},\n",
       "   {'trialId': '2',\n",
       "    'hyperparameters': {'alpha': '0.00028720669188095251', 'max_iter': '200'},\n",
       "    'startTime': '2023-03-30T11:47:19.782421920Z',\n",
       "    'endTime': '2023-03-30T11:55:45Z',\n",
       "    'state': 'FAILED'},\n",
       "   {'trialId': '3',\n",
       "    'hyperparameters': {'alpha': '0.00052478461130302679', 'max_iter': '300'},\n",
       "    'startTime': '2023-03-30T11:47:19.782470259Z',\n",
       "    'endTime': '2023-03-30T11:55:34Z',\n",
       "    'state': 'FAILED'},\n",
       "   {'trialId': '4',\n",
       "    'hyperparameters': {'alpha': '0.0002479662247784391', 'max_iter': '200'},\n",
       "    'startTime': '2023-03-30T11:47:19.782504104Z',\n",
       "    'endTime': '2023-03-30T11:55:34Z',\n",
       "    'state': 'FAILED'}],\n",
       "  'consumedMLUnits': 0.26,\n",
       "  'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'accuracy'},\n",
       " 'etag': 'j7P4KSdvS9M=',\n",
       " 'jobPosition': '0'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned run results are sorted by a value of the optimization metric. The best run is the first item on the returned list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trialId': '1',\n",
       " 'hyperparameters': {'alpha': '0.000505', 'max_iter': '300'},\n",
       " 'startTime': '2023-03-30T11:47:19.782241508Z',\n",
       " 'endTime': '2023-03-30T11:55:45Z',\n",
       " 'state': 'FAILED'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model with the best hyperparameters\n",
    "\n",
    "You can now retrain the model using the best hyperparameters and using combined training and validation splits as a training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = response['trainingOutput']['trials'][0]['hyperparameters']['alpha']\n",
    "max_iter = response['trainingOutput']['trials'][0]['hyperparameters']['max_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20230330_115756] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20230330_115756\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20230330_115756\n",
      "jobId: JOB_20230330_115756\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--validation_dataset_path=$VALIDATION_FILE_PATH \\\n",
    "--alpha=$alpha \\\n",
    "--max_iter=$max_iter \\\n",
    "--nohptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2023-03-30 11:57:58 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2023-03-30 11:57:58 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2023-03-30 11:57:58 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2023-03-30 11:57:59 +0000\tservice\t\tJob JOB_20230330_115756 is queued.\n",
      "INFO\t2023-03-30 11:58:00 +0000\tservice\t\tWaiting for training program to start.\n",
      "NOTICE\t2023-03-30 11:58:31 +0000\tmaster-replica-0.gcsfuse\t\tOpening GCS connection...\n",
      "NOTICE\t2023-03-30 11:58:31 +0000\tmaster-replica-0.gcsfuse\t\tMounting file system \"gcsfuse\"...\n",
      "NOTICE\t2023-03-30 11:58:31 +0000\tmaster-replica-0.gcsfuse\t\tFile system has been successfully mounted.\n",
      "ERROR\t2023-03-30 12:00:20 +0000\tmaster-replica-0\t\t--training_dataset_path=gs://qwiklabs-gcp-03-b8978ad3fa8f-kubeflowpipelines-default/data/training/dataset.csv: 1: [python,: not found\n",
      "ERROR\t2023-03-30 12:00:42 +0000\tservice\t\tThe replica master 0 exited with a non-zero status of 127. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=354076792935&resource=ml_job%2Fjob_id%2FJOB_20230330_115756&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22JOB_20230330_115756%22\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: The above AI platform job stream logs will take approximately 5~10 minutes to display.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the training output\n",
    "\n",
    "The training script saved the trained model as the 'model.pkl' in the `JOB_DIR` folder on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to AI Platform Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the `gcloud` command below to create a model with\n",
    "`model_name` in `$REGION` tagged with `labels`:\n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-01-caip-containers** and opening **lab-01.ipynb**.\n",
    "</ql-infobox>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Created ai platform model [projects/qwiklabs-gcp-03-b8978ad3fa8f/models/forest_cover_classifier2].\n"
     ]
    }
   ],
   "source": [
    "model_name = 'forest_cover_classifier'\n",
    "labels = \"task=classifier,domain=forestry\"\n",
    "\n",
    "!gcloud ai-platform models create forest_cover_classifier2 \\\n",
    " --regions=$REGION \\\n",
    " --labels=$labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `gcloud` command below to create a version of the model:\n",
    "\n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-01-caip-containers** and opening **lab-01.ipynb**.\n",
    "</ql-infobox>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.versions.create) NOT_FOUND: Field: parent Error: The model resource: (354076792935, forest_cover_classifier2) was not found. Please create the model resource first by using 'gcloud ai-platform models create forest_cover_classifier2'.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: \"The model resource: (354076792935, forest_cover_classifier2) was\\\n",
      "      \\ not found. Please create the model resource first by using 'gcloud ai-platform\\\n",
      "      \\ models create forest_cover_classifier2'.\"\n",
      "    field: parent\n"
     ]
    }
   ],
   "source": [
    "model_version = 'v01'\n",
    "\n",
    "!gcloud ai-platform versions create {model_version} \\\n",
    "--model=forest_cover_classifier2 \\\n",
    "--origin=$JOB_DIR \\\n",
    "--runtime-version=1.15 \\\n",
    "--framework=scikit-learn \\\n",
    "--python-version=3.7 \\\n",
    "--region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-central1\n",
      "gs://qwiklabs-gcp-03-b8978ad3fa8f-kubeflowpipelines-default/jobs/JOB_20230330_115756\n"
     ]
    }
   ],
   "source": [
    "!echo $REGION\n",
    "!echo $JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve predictions\n",
    "#### Prepare the input file with JSON formated instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'serving_instances.json'\n",
    "\n",
    "with open(input_file, 'w') as f:\n",
    "    for index, row in X_validation.head().iterrows():\n",
    "        f.write(json.dumps(list(row.values)))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3187.0, 51.0, 50.0, 124.0, 106.0, 3995.0, 186.0, 85.0, 0.0, 931.0, \"Rawah\", \"C7746\"]\n",
      "[2797.0, 13.0, 48.0, 242.0, 42.0, 1082.0, 129.0, 98.0, 68.0, 1006.0, \"Commanche\", \"C7755\"]\n",
      "[3186.0, 46.0, 45.0, 150.0, 105.0, 4011.0, 189.0, 104.0, 7.0, 911.0, \"Rawah\", \"C7746\"]\n",
      "[1972.0, 43.0, 44.0, 67.0, 62.0, 95.0, 186.0, 107.0, 15.0, 351.0, \"Cache\", \"C2702\"]\n",
      "[3155.0, 52.0, 45.0, 95.0, 74.0, 3967.0, 199.0, 107.0, 0.0, 924.0, \"Rawah\", \"C7746\"]\n"
     ]
    }
   ],
   "source": [
    "!cat $input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Using the `gcloud` command send the data in `$input_file` to \n",
    "your model deployed as a REST API:\n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-01-caip-containers** and opening **lab-01.ipynb**.\n",
    "</ql-infobox>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "\u001b[1;31mERROR:\u001b[0m gcloud crashed (AttributeError): 'NoneType' object has no attribute 'framework'\n",
      "\n",
      "If you would like to report this issue, please run the following command:\n",
      "  gcloud feedback\n",
      "\n",
      "To check gcloud for common problems, please run the following command:\n",
      "  gcloud info --run-diagnostics\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "--model forest_cover_classifier2\\\n",
    "--json-instances $input_file \\\n",
    "--region global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
